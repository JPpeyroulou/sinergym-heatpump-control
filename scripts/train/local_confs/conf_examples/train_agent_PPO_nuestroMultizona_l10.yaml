experiment_name: Eplus-PPO-nuestroMultizona-l10
environment: Eplus-nuestroMultizonaPPO_l10-uru-continuous-v1
episodes: 20

algorithm:
  name: stable_baselines3:PPO
  log_interval: 1
  parameters:
    policy: MlpPolicy
    learning_rate: 0.0003
    n_steps: 2048
    batch_size: 256
    n_epochs: 20
    gamma: 0.99
    gae_lambda: 0.95
    clip_range: 0.2
    clip_range_vf: null
    normalize_advantage: true
    ent_coef: 0.01
    vf_coef: 0.5
    max_grad_norm: 0.5
    use_sde: false
    sde_sample_freq: -1
    rollout_buffer_class: null
    rollout_buffer_kwargs: null
    target_kl: null
    stats_window_size: 100
    tensorboard_log: null
    policy_kwargs: {net_arch: [256, 256]}
    verbose: 1
    seed: null
    device: auto
    _init_setup_model: true

evaluation:
  eval_freq: 1
  eval_length: 1

wrappers_yaml_config: null
wrappers:
  - sinergym.utils.wrappers:NormalizeAction: {}
  - sinergym.utils.wrappers:NormalizeObservation: {}
  - sinergym.utils.wrappers:LoggerWrapper:
      storage_class: sinergym.utils.logger:LoggerStorage
  - sinergym.utils.wrappers:CSVLogger: {}

cloud: 
  remote_store: null
  auto_delete: null
