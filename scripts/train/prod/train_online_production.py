#!/usr/bin/env python3
"""
Sistema de Aprendizaje Online en Producci√≥n
Entrenamiento en tiempo real con entornos de producci√≥n personalizados
"""

import os
import sys
import yaml
import time
import json
import numpy as np
from datetime import datetime
from typing import Dict, Any, Optional, Tuple
import warnings

# ============ REGISTRAR ENTORNO PERSONALIZADO ============
# Importar y registrar antes de cualquier otra cosa
import sys
import os

# A√±adir el directorio actual al path
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

# Importar el archivo de registro (ejecuta el registro autom√°ticamente)
try:
    from register_production_envs import *
    print("‚úÖ Entorno de producci√≥n registrado")
except ImportError as e:
    print(f"‚ö†Ô∏è  No se pudo registrar el entorno: {e}")
    # Intentar crear el entorno directamente
    pass

# ============ IMPORTS PRINCIPALES ============
import gymnasium as gym
import sinergym
from sinergym.utils.common import create_environment as create_sinergym_env, import_from_path
from sinergym.utils.wrappers import LoggerWrapper, NormalizeObservation, MultiObsWrapper
from sinergym.utils.rewards import NuestroRewardMultizona

# Stable Baselines3
try:
    from stable_baselines3 import SAC, PPO, TD3, DDPG
    from stable_baselines3.common.callbacks import CheckpointCallback, EvalCallback, BaseCallback
    from stable_baselines3.common.vec_env import DummyVecEnv, SubprocVecEnv
    from stable_baselines3.common.monitor import Monitor
    HAS_SB3 = True
except ImportError:
    HAS_SB3 = False
    print("‚ö†Ô∏è  Stable Baselines3 no encontrado")

# ============ CONFIGURACI√ìN ============
def load_config(config_path: str) -> Dict[str, Any]:
    """Carga configuraci√≥n desde archivo YAML"""
    with open(config_path, 'r') as f:
        config = yaml.safe_load(f)
    return config

def save_config(config: Dict[str, Any], path: str):
    """Guarda configuraci√≥n a archivo"""
    with open(path, 'w') as f:
        yaml.dump(config, f, default_flow_style=False)

# ============ CREACI√ìN DE ENTORNO ============
def create_production_env(config: Dict[str, Any]) -> gym.Env:
    """Crea el entorno de producci√≥n"""
    env_name = config['environment']
    
    print(f"\nüéØ Creando entorno: {env_name}")
    print(f"   Tipo: {config.get('env_type', 'production')}")
    
    # Verificar si es un entorno de producci√≥n personalizado
    if 'multizona-production' in env_name or env_name == 'Eplus-pyenv-multizona-production-v1':
        # ===== ENTORNO DE PRODUCCI√ìN PERSONALIZADO =====
        try:
            # Importar nuestra clase personalizada
            from pyenv_production import PyEnvProduction
            
            # Obtener configuraci√≥n del entorno
            env_config = config.get('env_config', {})
            
            # Configuraci√≥n por defecto para producci√≥n
            default_config = {
                'building_file': 'OfficeMedium_Zone_4.pkl',
                'weather_files': ['USA_CO_Denver.Intl.AP.725650_TMY3.epw'],
                # IMPORTANTE: variables y meters deben venir del YAML, no del default
                # Dejarlos vac√≠os para evitar mezclar con las del YAML
                'variables': {},
                'meters': {},
                'actuators': {
                    'heating_setpoint_1': ('Schedule:Compact', 'Schedule Value', 'Heating Setpoint 1'),
                    'cooling_setpoint_1': ('Schedule:Compact', 'Schedule Value', 'Cooling Setpoint 1'),
                    'heating_setpoint_2': ('Schedule:Compact', 'Schedule Value', 'Heating Setpoint 2'),
                    'cooling_setpoint_2': ('Schedule:Compact', 'Schedule Value', 'Cooling Setpoint 2'),
                    'heating_setpoint_3': ('Schedule:Compact', 'Schedule Value', 'Heating Setpoint 3'),
                    'cooling_setpoint_3': ('Schedule:Compact', 'Schedule Value', 'Cooling Setpoint 3'),
                    'heating_setpoint_4': ('Schedule:Compact', 'Schedule Value', 'Heating Setpoint 4'),
                    'cooling_setpoint_4': ('Schedule:Compact', 'Schedule Value', 'Cooling Setpoint 4')
                },
                'action_space': gym.spaces.Box(
                    low=np.array([-20.0, 22.0, 22.0, 22.0, 22.0, 22.0, 22.0, 22.0, 22.0], dtype=np.float32),
                    high=np.array([30.0, 26.0, 26.0, 26.0, 26.0, 26.0, 26.0, 26.0, 26.0], dtype=np.float32),
                    dtype=np.float32
                ),
                'reward': NuestroRewardMultizona,  # ‚úÖ Usar NuestroRewardMultizona por defecto
                'reward_kwargs': {
                    'temperature_variables': ['zone_air_temperature_1', 'zone_air_temperature_2', 
                                            'zone_air_temperature_3', 'zone_air_temperature_4'],
                    'humidity_variables': [],  # Agregar si es necesario
                    'energy_variables': ['total_electric_demand'],  # Debe ser lista
                    'energy_weight': 0.5,
                    'lambda_energy': 1.0,
                    'lambda_temperature': 30,
                    'high_price': 14.493,
                    'low_price': 4.556,
                    'schedule_csv': None
                },
                'env_name': env_name,
                'time_variables': ['month', 'day_of_month', 'hour'],  # ‚úÖ Necesario para NuestroRewardMultizona
                'config_params': {
                    'timesteps_per_hour': 12,  # 12 pasos por hora (cada paso = 5 minutos)
                    'runperiod': (1, 1, 12, 31),
                    'action_definition': {}
                },
                'production_config': {
                    'data_mode': 'homeassistant',  # Cambiado a homeassistant por defecto
                    'action_delay': 1.0,
                    'max_steps_per_episode': 288,  # 24 horas = 288 pasos (12 pasos/hora, 5 min/paso)
                    'safety_limits': {
                        'max_zone_temperature': 28.0,
                        'min_zone_temperature': 16.0
                    }
                }
            }
            
            # Combinar configuraci√≥n por defecto con la proporcionada
            import copy
            final_config = copy.deepcopy(default_config)
            
            # Actualizar recursivamente, pero para variables y meters, reemplazar completamente
            def deep_update(d, u):
                for k, v in u.items():
                    # Para variables y meters, reemplazar completamente (no merge)
                    if k in ['variables', 'meters'] and isinstance(v, dict):
                        d[k] = copy.deepcopy(v)
                    # Para production_config, hacer merge profundo pero preservar data_mode del YAML
                    elif k == 'production_config' and isinstance(v, dict) and k in d and isinstance(d[k], dict):
                        # Preservar data_mode del YAML si existe
                        yaml_data_mode = v.get('data_mode')
                        deep_update(d[k], v)
                        if yaml_data_mode:
                            d[k]['data_mode'] = yaml_data_mode
                    elif isinstance(v, dict) and k in d and isinstance(d[k], dict):
                        deep_update(d[k], v)
                    else:
                        d[k] = v
            
            deep_update(final_config, env_config)
            
            # Procesar reward si viene como string (desde YAML)
            if isinstance(final_config.get('reward'), str):
                try:
                    final_config['reward'] = import_from_path(final_config['reward'])
                    print(f"   ‚úÖ Recompensa importada: {final_config['reward'].__name__}")
                except Exception as e:
                    print(f"   ‚ö†Ô∏è  Error importando recompensa: {e}, usando NuestroRewardMultizona")
                    final_config['reward'] = NuestroRewardMultizona
            
            # Procesar action_space si viene como dict (desde YAML)
            if isinstance(final_config.get('action_space'), dict):
                action_space_dict = final_config['action_space']
                if 'low' in action_space_dict and 'high' in action_space_dict:
                    final_config['action_space'] = gym.spaces.Box(
                        low=np.array(action_space_dict['low'], dtype=np.float32),
                        high=np.array(action_space_dict['high'], dtype=np.float32),
                        dtype=np.float32
                    )
                    print(f"   ‚úÖ Action space procesado desde YAML: {len(action_space_dict['low'])} dimensiones")
                elif isinstance(final_config.get('action_space'), str):
                    # Si viene como string (c√≥digo Python), evaluarlo
                    final_config['action_space'] = eval(final_config['action_space'])
                    print(f"   ‚úÖ Action space procesado desde YAML (string): {final_config['action_space'].shape[0]} dimensiones")
            
            # Procesar variables si vienen en formato YAML (lista [energyplus_var_name, key])
            # Convertir al formato Sinergym que espera PyEnvProduction
            if isinstance(final_config.get('variables'), dict):
                print(f"\nüîç DEBUG: Procesando variables del YAML...")
                print(f"   Variables en YAML (antes de procesar): {list(final_config['variables'].keys())}")
                # IMPORTANTE: Limpiar variables_processed para evitar mezclar con defaults
                variables_processed = {}
                
                for var_name, var_spec in final_config['variables'].items():
                    if isinstance(var_spec, list) and len(var_spec) >= 2:
                        # Formato YAML: [energyplus_var_name, key]
                        # Convertir a formato Sinergym: {energyplus_var_name: {'variable_names': var_name, 'keys': key}}
                        eplus_var_name = var_spec[0]
                        key = var_spec[1]
                        
                        # Si ya existe esta variable de EnergyPlus, agregar a la lista de keys
                        if eplus_var_name in variables_processed:
                            existing_keys = variables_processed[eplus_var_name]['keys']
                            if isinstance(existing_keys, list):
                                existing_keys.append(key)
                            else:
                                variables_processed[eplus_var_name]['keys'] = [existing_keys, key]
                        else:
                            variables_processed[eplus_var_name] = {
                                'variable_names': var_name,  # Nombre que se usar√° en observaciones
                                'keys': key
                            }
                    
                    elif isinstance(var_spec, tuple) and len(var_spec) >= 2:
                        # Formato tupla: (energyplus_var_name, key)
                        eplus_var_name = var_spec[0]
                        key = var_spec[1]
                        
                        if eplus_var_name in variables_processed:
                            existing_keys = variables_processed[eplus_var_name]['keys']
                            if isinstance(existing_keys, list):
                                existing_keys.append(key)
                            else:
                                variables_processed[eplus_var_name]['keys'] = [existing_keys, key]
                        else:
                            variables_processed[eplus_var_name] = {
                                'variable_names': var_name,
                                'keys': key
                            }
                    
                    elif isinstance(var_spec, dict):
                        # Ya est√° en formato Sinergym
                        variables_processed[var_name] = var_spec
                    else:
                        # Mantener como est√°
                        variables_processed[var_name] = var_spec
                
                final_config['variables'] = variables_processed
                print(f"   ‚úÖ Variables procesadas: {len(variables_processed)} variables")
                print(f"   Variables finales: {list(variables_processed.keys())}")
            
            # Procesar meters: Sinergym invierte el formato (EnergyPlus name -> variable name)
            # Formato YAML: "Heat Pump:Heating:Electricity: total_electricity_HVAC"
            # Formato esperado: {"total_electricity_HVAC": "Heat Pump:Heating:Electricity"}
            if 'meters' in final_config and isinstance(final_config['meters'], dict):
                print(f"\nüîç DEBUG: Procesando meters del YAML...")
                print(f"   Meters en YAML (antes de procesar): {final_config['meters']}")
                # IMPORTANTE: Limpiar meters_processed para evitar mezclar con defaults
                meters_processed = {}
                for eplus_name, var_name in final_config['meters'].items():
                    # Si el formato es invertido (como en nuestroMultrizona.yaml)
                    # eplus_name = "Heat Pump:Heating:Electricity", var_name = "total_electricity_HVAC"
                    meters_processed[var_name] = eplus_name
                final_config['meters'] = meters_processed
                print(f"   ‚úÖ Meters procesados: {len(meters_processed)} meters")
                print(f"   Meters finales: {meters_processed}")
            
            # Crear entorno directamente (sin gym.make para m√°s control)
            env = PyEnvProduction(
                building_file=final_config['building_file'],
                weather_files=final_config['weather_files'],
                variables=final_config['variables'],
                meters=final_config['meters'],
                actuators=final_config['actuators'],
                action_space=final_config['action_space'],
                reward=final_config['reward'],
                reward_kwargs=final_config['reward_kwargs'],
                env_name=final_config['env_name'],
                config_params=final_config['config_params'],
                production_config=final_config['production_config'],
                time_variables=final_config.get('time_variables', ['month', 'day_of_month', 'hour'])  # ‚úÖ Necesario para NuestroRewardMultizona
            )
            
            print(f"‚úÖ Entorno de producci√≥n creado:")
            print(f"   - Variables: {len(env.variable_names)}")
            print(f"   - Modo datos: {env.production_config.get('data_mode', 'N/A')}")
            # Verificar que action_space sea un objeto Box antes de acceder a shape
            if hasattr(env.action_space, 'shape') and len(env.action_space.shape) > 0:
                print(f"   - Acciones: {env.action_space.shape[0]} dimensiones")
            else:
                print(f"   - Acciones: {env.action_space}")
            
            return env
            
        except Exception as e:
            print(f"‚ùå Error creando entorno de producci√≥n: {e}")
            raise
    
    else:
        # ===== ENTORNO NORMAL DE SINERGYM =====
        try:
            # Usar la funci√≥n est√°ndar de Sinergym
            env = create_sinergym_env(
                id_env=env_name,
                **config.get('env_kwargs', {})
            )
            print(f"‚úÖ Entorno Sinergym creado: {env_name}")
            return env
        except KeyError:
            # Intentar con gym.make
            try:
                env = gym.make(env_name)
                print(f"‚úÖ Entorno Gymnasium creado: {env_name}")
                return env
            except Exception as e:
                print(f"‚ùå Error creando entorno {env_name}: {e}")
                raise

def apply_wrappers(env: gym.Env, config: Dict[str, Any]) -> gym.Env:
    """Aplica wrappers al entorno"""
    wrappers = config.get('wrappers', [])
    
    print(f"\nüéÅ Aplicando wrappers ({len(wrappers)} configurados):")
    
    # Lista de wrappers ya aplicados para evitar duplicados
    applied_wrappers = set()
    
    # Aplicar wrappers personalizados desde configuraci√≥n
    for wrapper_cfg in wrappers:
        wrapper_name = wrapper_cfg['name']
        wrapper_params = wrapper_cfg.get('params', {})
        
        # Evitar aplicar el mismo wrapper dos veces
        if wrapper_name in applied_wrappers:
            print(f"   ‚ö†Ô∏è  {wrapper_name} ya aplicado, omitiendo")
            continue
        
        try:
            # Importar din√°micamente
            module = __import__('sinergym.utils.wrappers', fromlist=[wrapper_name])
            wrapper_class = getattr(module, wrapper_name)
            
            # LoggerWrapper no acepta par√°metros como 'flag', solo storage_class
            if wrapper_name == 'LoggerWrapper':
                # Remover par√°metros no v√°lidos
                wrapper_params = {k: v for k, v in wrapper_params.items() if k == 'storage_class'}
                if wrapper_params:
                    env = wrapper_class(env, **wrapper_params)
                else:
                    env = wrapper_class(env)
            else:
                # Aplicar wrapper con par√°metros
                env = wrapper_class(env, **wrapper_params)
            
            applied_wrappers.add(wrapper_name)
            print(f"   ‚úÖ {wrapper_name}")
            
        except Exception as e:
            print(f"   ‚ö†Ô∏è  No se pudo aplicar {wrapper_name}: {e}")
    
    # Wrapper de normalizaci√≥n de observaciones (si no est√° en la lista y est√° habilitado)
    if config.get('normalize', False) and 'NormalizeObservation' not in applied_wrappers:
        env = NormalizeObservation(env)
        print("   ‚úÖ NormalizeObservation")
    
    # Wrapper de normalizaci√≥n de acciones (para que coincida con el modelo entrenado)
    # IMPORTANTE: Aplicar NormalizeAction para que el action_space sea [-1, 1] como el modelo
    # Esto es CR√çTICO para que el modelo pueda entrenarse online
    if 'NormalizeAction' not in applied_wrappers:
        try:
            from sinergym.utils.wrappers import NormalizeAction
            # Verificar que el action_space sea Box antes de aplicar
            if isinstance(env.action_space, gym.spaces.Box):
                env = NormalizeAction(env, normalize_range=(-1.0, 1.0))
                print("   ‚úÖ NormalizeAction (action_space normalizado a [-1, 1])")
                print(f"      Action space original: {env.real_space if hasattr(env, 'real_space') else 'N/A'}")
                print(f"      Action space normalizado: {env.action_space}")
                applied_wrappers.add('NormalizeAction')
            else:
                print(f"   ‚ö†Ô∏è  Action space no es Box, no se puede aplicar NormalizeAction")
        except ImportError:
            print("   ‚ö†Ô∏è  NormalizeAction no disponible, el action_space puede no coincidir con el modelo")
        except Exception as e:
            print(f"   ‚ö†Ô∏è  No se pudo aplicar NormalizeAction: {e}")
            import traceback
            traceback.print_exc()
    
    # Wrapper de logger (si no est√° en la lista y est√° habilitado)
    if config.get('logging', True) and 'LoggerWrapper' not in applied_wrappers:
        env = LoggerWrapper(env)
        print(f"   ‚úÖ LoggerWrapper (logs en: {env.get_wrapper_attr('workspace_path')})")
    
    return env

# ============ DESNORMALIZACI√ìN DE ACCIONES ============
def adapt_action_dimensions(action: np.ndarray, target_dim: int, 
                           target_low: np.ndarray, target_high: np.ndarray,
                           verbose: bool = False) -> np.ndarray:
    """
    Adapta una acci√≥n a la dimensi√≥n objetivo.
    
    Args:
        action: Acci√≥n del modelo
        target_dim: Dimensi√≥n objetivo
        target_low: L√≠mites inferiores del espacio objetivo
        target_high: L√≠mites superiores del espacio objetivo
        verbose: Si True, imprime informaci√≥n
        
    Returns:
        Acci√≥n adaptada a la dimensi√≥n objetivo
    """
    current_dim = action.shape[0] if len(action.shape) > 0 else len(action)
    
    if current_dim == target_dim:
        return action
    elif current_dim > target_dim:
        # Reducir: tomar las primeras target_dim dimensiones
        if verbose:
            print(f"      ‚ö†Ô∏è  Reduciendo acci√≥n de {current_dim} a {target_dim} dimensiones")
        return action[:target_dim]
    else:
        # Aumentar: necesitamos mapear las dimensiones del modelo a las del entorno
        if verbose:
            print(f"      ‚ö†Ô∏è  Expandindo acci√≥n de {current_dim} a {target_dim} dimensiones")
            print(f"      ‚ö†Ô∏è  ADVERTENCIA: No hay mapeo definido para dimensiones faltantes")
            print(f"      ‚ö†Ô∏è  Las dimensiones {current_dim}-{target_dim-1} se rellenar√°n con valores por defecto")
            print(f"      ‚ö†Ô∏è  Esto es INCORRECTO - necesitas definir el mapeo en la configuraci√≥n")
        
        adapted = np.zeros(target_dim, dtype=action.dtype)
        
        # Mapeo b√°sico: usar las primeras dimensiones del modelo para las primeras del entorno
        # Esto es solo un placeholder - DEBES definir el mapeo correcto seg√∫n tus actuadores
        min_dim = min(current_dim, target_dim)
        adapted[:min_dim] = action[:min_dim]
        
        # Rellenar las dimensiones faltantes con valores por defecto (punto medio)
        # ‚ö†Ô∏è ESTO ES INCORRECTO - solo es un placeholder
        for i in range(min_dim, target_dim):
            default_value = (target_low[i] + target_high[i]) / 2.0
            adapted[i] = default_value
            if verbose:
                print(f"         ‚ö†Ô∏è  Dimensi√≥n {i}: rellenada con {default_value:.1f} (VALOR POR DEFECTO - INCORRECTO)")
        
        if verbose:
            print(f"\n      ‚ùå ERROR: El modelo tiene {current_dim} dimensiones pero el entorno necesita {target_dim}")
            print(f"      ‚ùå Las dimensiones {min_dim}-{target_dim-1} est√°n rellenas con valores por defecto")
            print(f"      ‚ùå DEBES definir un mapeo correcto en la configuraci√≥n o usar el mismo action_space")
        
        return adapted

def denormalize_action(action: np.ndarray, action_space: gym.spaces.Box, 
                      model_action_space: Optional[gym.spaces.Box] = None,
                      verbose: bool = False) -> np.ndarray:
    """
    Desnormaliza una acci√≥n del rango del modelo al rango real del action_space.
    
    Args:
        action: Acci√≥n del modelo (puede estar normalizada)
        action_space: Espacio de acciones del entorno con los rangos reales
        model_action_space: Espacio de acciones del modelo (opcional, para mapeo directo)
        verbose: Si True, imprime informaci√≥n de la desnormalizaci√≥n
        
    Returns:
        Acci√≥n desnormalizada en el rango real
    """
    if not isinstance(action_space, gym.spaces.Box):
        return action
    
    action = np.array(action, dtype=np.float32)
    target_low = action_space.low
    target_high = action_space.high
    target_dim = action_space.shape[0]
    action_dim = action.shape[0] if len(action.shape) > 0 else len(action)
    
    # Verificar y adaptar dimensiones si es necesario
    if action_dim != target_dim:
        if verbose:
            print(f"      ‚ö†Ô∏è  Dimensiones no coinciden: modelo={action_dim}, entorno={target_dim}")
        action = adapt_action_dimensions(action, target_dim, target_low, target_high, verbose)
    
    # Inicializar action_real como None para indicar que a√∫n no se ha procesado
    action_real = None
    
    # Si tenemos el action_space del modelo, intentar mapear directamente
    if model_action_space is not None and isinstance(model_action_space, gym.spaces.Box):
        source_low = model_action_space.low
        source_high = model_action_space.high
        
        # Verificar que las dimensiones coincidan despu√©s de la adaptaci√≥n
        if len(source_low) == len(action) and len(target_low) == len(action) and len(source_high) == len(action):
            # Mapear desde el rango del modelo al rango del entorno
            action_real = target_low + (action - source_low) / (source_high - source_low + 1e-8) * (target_high - target_low)
            
            if verbose:
                print(f"      üîÑ Mapeando desde modelo [{np.min(source_low):.1f}, {np.max(source_high):.1f}]")
                print(f"         a entorno [{np.min(target_low):.1f}, {np.max(target_high):.1f}]")
        else:
            # Si las dimensiones no coinciden, usar detecci√≥n autom√°tica
            if verbose:
                print(f"      ‚ö†Ô∏è  Dimensiones no coinciden para mapeo directo (modelo={len(source_low)}, acci√≥n={len(action)}, entorno={len(target_low)})")
                print(f"      üîÑ Usando detecci√≥n autom√°tica de rango")
            # Continuar con la detecci√≥n autom√°tica m√°s abajo
            action_real = None
    
    # Si no se pudo mapear directamente, usar detecci√≥n autom√°tica
    if action_real is None:
        # Detectar autom√°ticamente el rango de la acci√≥n
        action_min = np.min(action)
        action_max = np.max(action)
        
        if verbose:
            print(f"      üìä Rango acci√≥n recibida: [{action_min:.3f}, {action_max:.3f}]")
            print(f"      üìä Rango esperado: [{np.min(target_low):.1f}, {np.max(target_high):.1f}]")
        
        # Verificar si ya est√° en el rango correcto (con tolerancia)
        tolerance = 0.1
        if np.all(action >= target_low - tolerance) and np.all(action <= target_high + tolerance):
            if verbose:
                print(f"      ‚úÖ Acci√≥n ya est√° en rango real, sin cambios")
            return np.clip(action, target_low, target_high).astype(np.float32)
        
        # Si la acci√≥n est√° en [-1, 1], desnormalizar a [target_low, target_high]
        if action_min >= -1.1 and action_max <= 1.1:
            # Normalizaci√≥n [-1, 1] -> [target_low, target_high]
            action_real = target_low + (action + 1.0) / 2.0 * (target_high - target_low)
            if verbose:
                print(f"      üîÑ Desnormalizando desde [-1, 1] a rango real")
        # Si la acci√≥n est√° en [0, 1], desnormalizar a [target_low, target_high]
        elif action_min >= -0.1 and action_max <= 1.1:
            # Normalizaci√≥n [0, 1] -> [target_low, target_high]
            action_real = target_low + action * (target_high - target_low)
            if verbose:
                print(f"      üîÑ Desnormalizando desde [0, 1] a rango real")
        else:
            # Por defecto, asumir que est√° en [-1, 1]
            action_real = target_low + (action + 1.0) / 2.0 * (target_high - target_low)
            if verbose:
                print(f"      üîÑ Desnormalizando (asumiendo [-1, 1]) a rango real")
    
    # Asegurar que est√© dentro de los l√≠mites
    action_real = np.clip(action_real, target_low, target_high)
    
    if verbose:
        print(f"      ‚úÖ Acci√≥n final: {action_real}")
    
    return action_real.astype(np.float32)

# ============ ADAPTACI√ìN DE OBSERVACIONES ============
class ObservationAdapterWrapper(gym.ObservationWrapper):
    """Wrapper que adapta observaciones a la dimensi√≥n esperada por el modelo"""
    
    def __init__(self, env: gym.Env, target_dim: int):
        super().__init__(env)
        self.target_dim = target_dim
        current_dim = env.observation_space.shape[0]
        
        # Actualizar espacio de observaci√≥n
        self.observation_space = gym.spaces.Box(
            low=env.observation_space.low[0] if current_dim > 0 else -np.inf,
            high=env.observation_space.high[0] if current_dim > 0 else np.inf,
            shape=(target_dim,),
            dtype=env.observation_space.dtype
        )
        
        print(f"   üîÑ ObservationAdapter: {current_dim} ‚Üí {target_dim} dimensiones")
    
    def observation(self, obs: np.ndarray) -> np.ndarray:
        """Adapta la observaci√≥n a la dimensi√≥n objetivo"""
        current_dim = obs.shape[0] if len(obs.shape) > 0 else len(obs)
        
        if current_dim == self.target_dim:
            return obs
        elif current_dim > self.target_dim:
            # Reducir: tomar las primeras target_dim dimensiones
            return obs[:self.target_dim]
        else:
            # Aumentar: rellenar con ceros
            adapted = np.zeros(self.target_dim, dtype=obs.dtype)
            adapted[:current_dim] = obs
            return adapted

def adapt_observation(obs: np.ndarray, target_dim: int) -> np.ndarray:
    """
    Adapta una observaci√≥n a la dimensi√≥n esperada por el modelo.
    (Funci√≥n auxiliar para uso manual)
    """
    current_dim = obs.shape[0] if len(obs.shape) > 0 else len(obs)
    
    if current_dim == target_dim:
        return obs
    elif current_dim > target_dim:
        return obs[:target_dim]
    else:
        adapted = np.zeros(target_dim, dtype=obs.dtype)
        adapted[:current_dim] = obs
        return adapted

# ============ CARGA DE MODELO ============
def load_production_model(model_path: str, config: Dict[str, Any], env: gym.Env) -> Tuple[Any, gym.Env]:
    """
    Carga un modelo preentrenado y adapta el entorno si es necesario.
    
    Returns:
        Tuple[model, env]: Modelo cargado y entorno (posiblemente adaptado)
    """
    """Carga un modelo preentrenado para producci√≥n"""
    print(f"\nü§ñ Cargando modelo desde: {model_path}")
    
    if not HAS_SB3:
        print("‚ùå Stable Baselines3 no est√° instalado")
        return None
    
    # Determinar tipo de algoritmo
    algo_config = config.get('algorithm', {})
    algo_name = algo_config.get('name', 'SAC')
    
    # Verificar espacios antes de cargar
    try:
        # Intentar cargar sin env primero para ver los espacios del modelo
        if algo_name == 'SAC':
            temp_model = SAC.load(model_path, device='cpu')
        elif algo_name == 'PPO':
            temp_model = PPO.load(model_path, device='cpu')
        elif algo_name == 'TD3':
            temp_model = TD3.load(model_path, device='cpu')
        elif algo_name == 'DDPG':
            temp_model = DDPG.load(model_path, device='cpu')
        else:
            temp_model = None
        
        if temp_model:
            model_obs_space = temp_model.observation_space
            env_obs_space = env.observation_space
            
            print(f"\nüìä Espacios de observaci√≥n:")
            print(f"   Modelo: {model_obs_space}")
            print(f"   Entorno: {env_obs_space}")
            
            # DEBUG: Desglose detallado
            print(f"\nüìä DESGLOSE DEL MODELO:")
            print(f"   Dimensiones: {model_obs_space.shape[0]}")
            print(f"   Shape: {model_obs_space.shape}")
            
            print(f"\nüìä DESGLOSE DEL ENTORNO:")
            print(f"   Dimensiones: {env_obs_space.shape[0]}")
            print(f"   Shape: {env_obs_space.shape}")
            
            if hasattr(env, 'time_variables'):
                print(f"   time_variables: {len(env.time_variables)} = {env.time_variables}")
            if hasattr(env, 'variable_names'):
                print(f"   variable_names: {len(env.variable_names)} = {env.variable_names}")
            if hasattr(env, 'meter_names'):
                print(f"   meter_names: {len(env.meter_names)} = {env.meter_names}")
            
            if model_obs_space.shape != env_obs_space.shape:
                print(f"\n‚ö†Ô∏è  ADVERTENCIA: Los espacios de observaci√≥n no coinciden!")
                print(f"   Modelo espera: {model_obs_space.shape[0]} dimensiones")
                print(f"   Entorno tiene: {env_obs_space.shape[0]} dimensiones")
                print(f"\n   üîÑ Aplicando ObservationAdapterWrapper para adaptar observaciones...")
                
                # Aplicar wrapper de adaptaci√≥n ANTES de cargar el modelo
                target_dim = model_obs_space.shape[0]
                env = ObservationAdapterWrapper(env, target_dim)
                print(f"   ‚úÖ Entorno adaptado: {env_obs_space.shape[0]} ‚Üí {target_dim} dimensiones")
                
                # Cargar sin env para evitar verificaci√≥n estricta
                if algo_name == 'SAC':
                    model = SAC.load(model_path, device='cpu')
                elif algo_name == 'PPO':
                    model = PPO.load(model_path, device='cpu')
                elif algo_name == 'TD3':
                    model = TD3.load(model_path, device='cpu')
                elif algo_name == 'DDPG':
                    model = DDPG.load(model_path, device='cpu')
                
                # Actualizar el entorno del modelo (ahora ya est√° adaptado)
                model.set_env(env)
                print(f"   ‚úÖ Modelo cargado y entorno adaptado correctamente")
            else:
                # Los espacios coinciden, cargar normalmente
                if algo_name == 'SAC':
                    model = SAC.load(model_path, env=env, verbose=1)
                elif algo_name == 'PPO':
                    model = PPO.load(model_path, env=env, verbose=1)
                elif algo_name == 'TD3':
                    model = TD3.load(model_path, env=env, verbose=1)
                elif algo_name == 'DDPG':
                    model = DDPG.load(model_path, env=env, verbose=1)
                print(f"   ‚úÖ Espacios coinciden, modelo cargado correctamente")
        else:
            # Fallback: cargar normalmente
            if algo_name == 'SAC':
                model = SAC.load(model_path, env=env, verbose=1)
            elif algo_name == 'PPO':
                model = PPO.load(model_path, env=env, verbose=1)
            elif algo_name == 'TD3':
                model = TD3.load(model_path, env=env, verbose=1)
            elif algo_name == 'DDPG':
                model = DDPG.load(model_path, env=env, verbose=1)
    
    except Exception as e:
        error_msg = str(e)
        print(f"‚ö†Ô∏è  Error verificando espacios: {e}")
        
        # Cargar modelo sin asignar entorno para evitar verificaci√≥n estricta
        if algo_name == 'SAC':
            model = SAC.load(model_path, device='cpu')
        elif algo_name == 'PPO':
            model = PPO.load(model_path, device='cpu')
        elif algo_name == 'TD3':
            model = TD3.load(model_path, device='cpu')
        elif algo_name == 'DDPG':
            model = DDPG.load(model_path, device='cpu')
        
        model_obs_dim = model.observation_space.shape[0]
        env_obs_dim = env.observation_space.shape[0]
        model_action_space = model.action_space
        env_action_space = env.action_space
        
        # Verificar si el problema es solo de action space (normalizado vs real)
        action_space_mismatch = (
            "Action spaces do not match" in error_msg or
            model_action_space.shape != env_action_space.shape or
            not np.allclose(model_action_space.low, env_action_space.low) or
            not np.allclose(model_action_space.high, env_action_space.high)
        )
        
        # Verificar si el problema es de observation space
        obs_space_mismatch = (
            "Observation spaces do not match" in error_msg or
            model_obs_dim != env_obs_dim
        )
        
        if action_space_mismatch and not obs_space_mismatch:
            print(f"   ‚ÑπÔ∏è  Action spaces diferentes (normalizado vs real) - esto es esperado")
            print(f"      Modelo: {model_action_space}")
            print(f"      Entorno: {env_action_space}")
            print(f"      Las acciones se desnormalizar√°n autom√°ticamente durante la ejecuci√≥n")
        
        if obs_space_mismatch:
            print(f"   ‚ö†Ô∏è  El modelo espera {model_obs_dim} dimensiones")
            print(f"   ‚ö†Ô∏è  El entorno proporciona {env_obs_dim} dimensiones")
            print(f"   üîÑ Aplicando wrapper de adaptaci√≥n de observaciones...")
            
            # Aplicar wrapper de adaptaci√≥n
            env = ObservationAdapterWrapper(env, model_obs_dim)
            print(f"   ‚úÖ Entorno adaptado: {env_obs_dim} ‚Üí {model_obs_dim} dimensiones")
        
        # Intentar asignar el entorno (puede fallar si los action spaces no coinciden, pero est√° bien)
        env_assigned = False
        try:
            model.set_env(env)
            print(f"   ‚úÖ Entorno asignado al modelo")
            env_assigned = True
        except Exception as e2:
            error_msg = str(e2)
            if "Action spaces do not match" in error_msg:
                print(f"   ‚ÑπÔ∏è  No se pudo asignar entorno (action spaces diferentes) - intentando con VecEnv")
            elif "Observation spaces do not match" in error_msg:
                print(f"   ‚ÑπÔ∏è  No se pudo asignar entorno (observation spaces diferentes) - intentando con VecEnv")
            else:
                print(f"   ‚ö†Ô∏è  No se pudo asignar entorno: {e2}")
                print(f"   ‚ö†Ô∏è  Intentando con VecEnv...")
            
            # Para entrenamiento, necesitamos asignar el entorno de otra manera
            # Usar VecEnv directamente si es necesario
            try:
                from stable_baselines3.common.vec_env import DummyVecEnv
                vec_env = DummyVecEnv([lambda: env])
                model.set_env(vec_env)
                print(f"   ‚úÖ Entorno asignado usando VecEnv")
                env_assigned = True
            except Exception as e3:
                error_msg3 = str(e3)
                # Si el problema es de action space, verificar si NormalizeAction est√° aplicado
                if "Action spaces do not match" in error_msg3:
                    print(f"   ‚ö†Ô∏è  Action space a√∫n no coincide despu√©s de VecEnv")
                    print(f"   ‚ÑπÔ∏è  Verificando si NormalizeAction est√° aplicado...")
                    # Verificar si el entorno tiene NormalizeAction
                    unwrapped = env
                    has_normalize_action = False
                    while hasattr(unwrapped, 'env'):
                        if hasattr(unwrapped, 'normalized_space'):
                            has_normalize_action = True
                            print(f"   ‚úÖ NormalizeAction detectado en el wrapper")
                            break
                        unwrapped = unwrapped.env
                    
                    if not has_normalize_action:
                        print(f"   ‚ö†Ô∏è  NormalizeAction no est√° aplicado - intentando aplicar ahora...")
                        try:
                            from sinergym.utils.wrappers import NormalizeAction
                            env = NormalizeAction(env, normalize_range=(-1.0, 1.0))
                            print(f"   ‚úÖ NormalizeAction aplicado")
                            # Intentar asignar de nuevo
                            vec_env = DummyVecEnv([lambda: env])
                            model.set_env(vec_env)
                            print(f"   ‚úÖ Entorno asignado despu√©s de aplicar NormalizeAction")
                            env_assigned = True
                        except Exception as e4:
                            print(f"   ‚ö†Ô∏è  No se pudo aplicar NormalizeAction: {e4}")
                            print(f"   ‚ö†Ô∏è  El modelo se usar√° sin entorno asignado (solo para testing)")
                    else:
                        print(f"   ‚ö†Ô∏è  NormalizeAction est√° aplicado pero a√∫n hay problemas")
                        print(f"   ‚ö†Ô∏è  El modelo se usar√° sin entorno asignado (solo para testing)")
                else:
                    print(f"   ‚ö†Ô∏è  No se pudo asignar entorno con VecEnv: {e3}")
                    print(f"   ‚ö†Ô∏è  El modelo se usar√° sin entorno asignado (solo para testing)")
        
        # Guardar flag para verificar antes de entrenar
        if not env_assigned:
            print(f"   ‚ö†Ô∏è  ADVERTENCIA: El entorno no pudo ser asignado al modelo")
            print(f"   ‚ö†Ô∏è  El entrenamiento fallar√°. Solo se puede usar para testing.")
        
        return model, env
    else:
        # Por defecto, SAC
        print(f"‚ö†Ô∏è  Algoritmo {algo_name} no reconocido, usando SAC")
        try:
            model = SAC.load(model_path, env=env, verbose=1)
        except Exception as e:
            print(f"   ‚ö†Ô∏è  Error cargando con entorno: {e}")
            model = SAC.load(model_path, device='cpu')
            model.set_env(env)
    
    print(f"‚úÖ Modelo cargado: {algo_name}")
    print(f"   - Entrada: {model.policy.observation_space.shape}")
    print(f"   - Salida: {model.policy.action_space.shape}")
    
    return model, env

# ============ WRAPPER PARA ACCIONES DETERMIN√çSTICAS ============
class DeterministicActionWrapper(gym.Wrapper):
    """
    Wrapper que fuerza acciones determin√≠sticas (0 exploraci√≥n) durante el entrenamiento.
    Intercepta las acciones en step() y las reemplaza por acciones determin√≠sticas del modelo.
    """
    def __init__(self, env: gym.Env, model):
        super(DeterministicActionWrapper, self).__init__(env)
        self.model = model
        self.current_obs = None
    
    def step(self, action):
        """
        Intercepta la acci√≥n y la reemplaza por una determin√≠stica basada en la observaci√≥n actual.
        """
        # Si tenemos la observaci√≥n actual, usar acci√≥n determin√≠stica
        if self.current_obs is not None:
            try:
                # Obtener acci√≥n determin√≠stica del modelo basada en la observaci√≥n actual
                deterministic_action, _ = self.model.predict(self.current_obs, deterministic=True)
                # Usar la acci√≥n determin√≠stica en lugar de la original
                action = deterministic_action
            except Exception as e:
                # Si falla, usar la acci√≥n original
                pass
        
        # Ejecutar step con la acci√≥n determin√≠stica
        obs, reward, terminated, truncated, info = self.env.step(action)
        
        # Guardar observaci√≥n actual para el pr√≥ximo step
        self.current_obs = obs.copy() if obs is not None else None
        
        return obs, reward, terminated, truncated, info
    
    def reset(self, **kwargs):
        """Reset del entorno y guardar observaci√≥n inicial."""
        obs, info = self.env.reset(**kwargs)
        self.current_obs = obs.copy() if obs is not None else None
        return obs, info

# ============ ENTRENAMIENTO ONLINE ============
def train_online(model, env: gym.Env, config: Dict[str, Any]):
    """Entrenamiento online en producci√≥n"""
    train_config = config.get('training', {})
    
    total_timesteps = train_config.get('total_timesteps', 10000)
    eval_freq = train_config.get('eval_freq', 1000)
    n_eval_episodes = train_config.get('n_eval_episodes', 2)
    save_freq = train_config.get('save_freq', 5000)
    enable_exploration = train_config.get('enable_exploration', True)  # Por defecto con exploraci√≥n
    
    print(f"\nüöÄ INICIANDO ENTRENAMIENTO ONLINE")
    print(f"   Timesteps totales: {total_timesteps}")
    print(f"   Frecuencia evaluaci√≥n: {eval_freq}")
    print(f"   Guardado cada: {save_freq} pasos")
    print(f"   Exploraci√≥n: {'‚úÖ ACTIVADA' if enable_exploration else '‚ùå DESACTIVADA (modo determin√≠stico)'}")
    print(f"   Hora inicio: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 60)
    
    # Directorio para checkpoints
    checkpoint_dir = f"./checkpoints/online/{datetime.now().strftime('%Y%m%d_%H%M%S')}"
    os.makedirs(checkpoint_dir, exist_ok=True)
    
    # Callback para guardar checkpoints
    checkpoint_callback = CheckpointCallback(
        save_freq=save_freq,
        save_path=checkpoint_dir,
        name_prefix='online_model'
    )
    
    # Entorno de evaluaci√≥n (clon del entorno principal)
    eval_env = create_production_env(config)
    eval_env = apply_wrappers(eval_env, config)
    
    # Si el entorno principal fue adaptado, aplicar el mismo wrapper al de evaluaci√≥n
    model_dim = model.observation_space.shape[0]
    eval_env_dim = eval_env.observation_space.shape[0]
    env_dim = env.observation_space.shape[0]
    
    # Si el entorno principal tiene dimensiones diferentes al modelo, fue adaptado
    if env_dim == model_dim and eval_env_dim != model_dim:
        # El entorno principal fue adaptado, aplicar el mismo wrapper al de evaluaci√≥n
        eval_env = ObservationAdapterWrapper(eval_env, model_dim)
        print(f"   üîÑ Entorno de evaluaci√≥n adaptado: {eval_env_dim} ‚Üí {model_dim} dimensiones")
    
    # Callback de evaluaci√≥n
    eval_callback = EvalCallback(
        eval_env,
        best_model_save_path=checkpoint_dir + '/best/',
        log_path=checkpoint_dir + '/logs/',
        eval_freq=eval_freq,
        n_eval_episodes=n_eval_episodes,
        deterministic=True,
        render=False,
        verbose=1
    )
    
    # Verificar que el entorno est√© asignado antes de entrenar
    if not hasattr(model, 'env') or model.env is None:
        print(f"   ‚ö†Ô∏è  El modelo no tiene entorno asignado. Intentando asignar...")
        try:
            from stable_baselines3.common.vec_env import DummyVecEnv
            vec_env = DummyVecEnv([lambda: env])
            model.set_env(vec_env)
            print(f"   ‚úÖ Entorno asignado antes de entrenar")
        except Exception as e:
            error_msg = str(e)
            if "Action spaces do not match" in error_msg or "Observation spaces do not match" in error_msg:
                # Intentar forzar la asignaci√≥n ignorando las diferencias de espacios
                print(f"   ‚ö†Ô∏è  Espacios diferentes detectados, intentando asignar de todas formas...")
                try:
                    # Crear un VecEnv que ignore las verificaciones estrictas
                    vec_env = DummyVecEnv([lambda: env])
                    # Asignar directamente sin verificaci√≥n
                    model.env = vec_env
                    print(f"   ‚úÖ Entorno asignado forzadamente (ignorando diferencias de espacios)")
                except Exception as e2:
                    print(f"   ‚ùå ERROR: No se pudo asignar el entorno al modelo: {e2}")
                    print(f"   ‚ùå No se puede entrenar sin entorno asignado")
                    raise RuntimeError("El modelo no tiene entorno asignado y no se pudo asignar") from e2
            else:
                print(f"   ‚ùå ERROR: No se pudo asignar el entorno al modelo: {e}")
                print(f"   ‚ùå No se puede entrenar sin entorno asignado")
                raise RuntimeError("El modelo no tiene entorno asignado y no se pudo asignar") from e
    
    # Configurar exploraci√≥n si est√° desactivada
    callbacks_list = [checkpoint_callback, eval_callback]
    
    if not enable_exploration:
        print(f"\n   üîí Modo determin√≠stico activado - 0 exploraci√≥n durante ejecuci√≥n")
        print(f"   ‚úÖ Las acciones ser√°n determin√≠sticas (sin exploraci√≥n)")
        print(f"   ‚úÖ El modelo seguir√° entren√°ndose con los datos recopilados")
        
        # Envolver el entorno para forzar acciones determin√≠sticas
        print(f"   üîÑ Aplicando wrapper de acciones determin√≠sticas...")
        env = DeterministicActionWrapper(env, model)
        
        # Reasignar el entorno al modelo con el wrapper
        try:
            from stable_baselines3.common.vec_env import DummyVecEnv
            vec_env = DummyVecEnv([lambda: env])
            model.set_env(vec_env)
            print(f"   ‚úÖ Entorno con wrapper determin√≠stico asignado")
        except Exception as e:
            print(f"   ‚ö†Ô∏è  No se pudo reasignar entorno: {e}")
            print(f"   ‚ö†Ô∏è  Continuando con entorno original")
        
        # Tambi√©n establecer ent_coef muy bajo como respaldo
        if hasattr(model, 'ent_coef'):
            original_ent_coef = model.ent_coef
            if isinstance(model.ent_coef, str) and model.ent_coef == "auto":
                model.ent_coef = 0.0001
            else:
                model.ent_coef = 0.0001
            print(f"   üìä ent_coef ajustado a {model.ent_coef} (original: {original_ent_coef})")
    
    # Entrenar
    try:
        model.learn(
            total_timesteps=total_timesteps,
            callback=callbacks_list,
            reset_num_timesteps=False,  # Continuar desde el modelo cargado
            log_interval=4,
            tb_log_name="online_production"
        )
        
        print(f"\n‚úÖ Entrenamiento online completado")
        print(f"   Checkpoints guardados en: {checkpoint_dir}")
        
    except KeyboardInterrupt:
        print("\n‚ö†Ô∏è  Entrenamiento interrumpido por usuario")
    except Exception as e:
        print(f"\n‚ùå Error durante entrenamiento: {e}")
        import traceback
        traceback.print_exc()
    
    finally:
        # Guardar modelo final
        final_model_path = os.path.join(checkpoint_dir, 'model_final.zip')
        model.save(final_model_path)
        print(f"   Modelo final guardado: {final_model_path}")
        
        # Cerrar entorno de evaluaci√≥n
        eval_env.close()
    
    return model

# ============ PRUEBA EN PRODUCCI√ìN ============
def run_production_test(model, env: gym.Env, config: Dict[str, Any]):
    """Ejecuta una prueba en producci√≥n sin aprendizaje"""
    test_config = config.get('testing', {})
    n_episodes = test_config.get('n_episodes', 1)
    max_steps = test_config.get('max_steps', 288)  # 24 horas = 288 pasos (12 pasos/hora, 5 min/paso)
    render = test_config.get('render', True)
    
    print(f"\nüß™ EJECUTANDO PRUEBA EN PRODUCCI√ìN")
    print(f"   Episodios: {n_episodes}")
    print(f"   Pasos m√°ximos por episodio: {max_steps}")
    
    results = []
    
    for episode in range(n_episodes):
        print(f"\nüìä Episodio de prueba #{episode + 1}")
        
        obs, info = env.reset()
        episode_reward = 0
        episode_steps = 0
        terminated = False
        truncated = False
        
        while not (terminated or truncated) and episode_steps < max_steps:
            # Predecir acci√≥n (el wrapper ya adapt√≥ la observaci√≥n)
            action_from_model, _states = model.predict(obs, deterministic=True)
            
            print(f"\n   üì• Acci√≥n del modelo (RAW, sin procesar):")
            print(f"      Valores originales: {action_from_model}")
            print(f"      Dimensiones: {len(action_from_model)}")
            
            # Obtener action_space del modelo
            model_action_space = getattr(model, 'action_space', None)
            if model_action_space is not None:
                print(f"\n   üìä Action space del MODELO (con el que fue entrenado):")
                print(f"      Dimensiones: {model_action_space.shape[0]}")
                print(f"      Low:  {model_action_space.low}")
                print(f"      High: {model_action_space.high}")
                print(f"      Rango por dimensi√≥n:")
                for i in range(model_action_space.shape[0]):
                    print(f"         [{i}] [{model_action_space.low[i]:.1f}, {model_action_space.high[i]:.1f}]")
                
                print(f"\n   üìä Action space del ENTORNO (lo que necesita ahora):")
                print(f"      Dimensiones: {env.action_space.shape[0]}")
                print(f"      Low:  {env.action_space.low}")
                print(f"      High: {env.action_space.high}")
                print(f"      Rango por dimensi√≥n:")
                for i in range(env.action_space.shape[0]):
                    print(f"         [{i}] [{env.action_space.low[i]:.1f}, {env.action_space.high[i]:.1f}]")
                
                # Verificar si realmente hay un problema
                dims_match = model_action_space.shape[0] == env.action_space.shape[0]
                ranges_match = (np.allclose(model_action_space.low, env.action_space.low) and 
                               np.allclose(model_action_space.high, env.action_space.high))
                
                if dims_match and ranges_match:
                    print(f"\n   ‚úÖ Action spaces coinciden perfectamente:")
                    print(f"      - Mismas dimensiones: {model_action_space.shape[0]}")
                    print(f"      - Mismos rangos: ‚úì")
                elif dims_match:
                    print(f"\n   ‚ö†Ô∏è  ADVERTENCIA:")
                    print(f"      Mismas dimensiones ({model_action_space.shape[0]}) pero rangos diferentes")
                    print(f"      Se ajustar√°n los rangos durante la desnormalizaci√≥n")
                else:
                    print(f"\n   ‚ùå PROBLEMA CR√çTICO:")
                    print(f"      El modelo fue entrenado con {model_action_space.shape[0]} dimensiones")
                    print(f"      pero el entorno de producci√≥n necesita {env.action_space.shape[0]} dimensiones")
                    print(f"      Son actuadores DIFERENTES - necesitas definir un mapeo correcto.")
                    print(f"\n   üí° SOLUCI√ìN:")
                    print(f"      1. Usar el mismo action_space del modelo en el entorno de producci√≥n")
                    print(f"      2. O definir un mapeo expl√≠cito de {model_action_space.shape[0]} ‚Üí {env.action_space.shape[0]}")
                    print(f"      3. O reentrenar el modelo con el action_space del entorno de producci√≥n")
            
            # PROBLEMA: NormalizeAction wrapper usa np.rint() que redondea a enteros
            # SOLUCI√ìN: Guardar la acci√≥n normalizada en info para que el bridge la use
            # En lugar de dejar que el wrapper la desnormalice incorrectamente
            
            # Verificar si el entorno tiene NormalizeAction wrapper
            has_normalize_action = False
            unwrapped_env = env
            while hasattr(unwrapped_env, 'env'):
                if hasattr(unwrapped_env, 'real_space'):
                    has_normalize_action = True
                    real_space = unwrapped_env.real_space
                    break
                unwrapped_env = unwrapped_env.env
            
            if has_normalize_action:
                # El wrapper NormalizeAction tiene problemas con np.rint()
                # Guardamos la acci√≥n normalizada en un atributo especial para que
                # el bridge la pueda usar directamente
                # Por ahora, pasamos la acci√≥n normalizada y el bridge intentar√° corregirla
                action = action_from_model
                print(f"   ‚úÖ Pasando acci√≥n normalizada (wrapper la procesar√°, bridge intentar√° corregir)")
            else:
                # No hay wrapper, desnormalizar manualmente
                action = denormalize_action(
                    action_from_model, 
                    env.action_space, 
                    model_action_space=model_action_space,
                    verbose=True
                )
            
            # Guardar acci√≥n normalizada antes de pasarla al wrapper
            # para que el bridge pueda usarla directamente (evitando np.rint)
            if has_normalize_action:
                # Guardar en el entorno sin wrapper para que el bridge lo pueda acceder
                try:
                    unwrapped_env = env
                    while hasattr(unwrapped_env, 'env'):
                        unwrapped_env = unwrapped_env.env
                    # Guardar la acci√≥n normalizada en el entorno base
                    unwrapped_env._last_normalized_action = action_from_model.copy()
                except Exception as e:
                    print(f"   ‚ö†Ô∏è  No se pudo guardar acci√≥n normalizada: {e}")
            
            # Ejecutar paso (el wrapper procesar√° la acci√≥n)
            obs, reward, terminated, truncated, info = env.step(action)
            
            # Guardar acci√≥n normalizada en info para debugging
            if has_normalize_action:
                info['action_normalized_original'] = action_from_model.tolist()
            
            episode_reward += reward
            episode_steps += 1
            
            if render:
                env.render()
            
            # Log cada 10 pasos
            if episode_steps % 10 == 0:
                print(f"   Paso {episode_steps}: Recompensa acumulada = {episode_reward:.2f}")
        
        # Guardar resultados del episodio
        episode_result = {
            'episode': episode + 1,
            'steps': episode_steps,
            'total_reward': float(episode_reward),
            'terminated': terminated,
            'truncated': truncated
        }
        results.append(episode_result)
        
        print(f"   ‚úÖ Episodio completado:")
        print(f"      - Pasos: {episode_steps}")
        print(f"      - Recompensa total: {episode_reward:.2f}")
    
    # Guardar resultados
    results_file = f"./logs/production_test_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    with open(results_file, 'w') as f:
        json.dump(results, f, indent=2)
    
    print(f"\nüìÑ Resultados guardados en: {results_file}")
    return results

# ============ FUNCI√ìN PRINCIPAL ============
def run_online_learning(config_path: str, model_path: Optional[str] = None):
    """Funci√≥n principal de aprendizaje online"""
    print("\n" + "="*70)
    print("üöÄ SISTEMA DE APRENDIZAJE ONLINE EN PRODUCCI√ìN")
    print("="*70)
    
    # 1. Cargar configuraci√≥n
    config = load_config(config_path)
    print(f"üìã Configuraci√≥n cargada: {config.get('name', 'sin_nombre')}")
    ha_cfg = config.get('env_config', {}).get('production_config', {}).get('api_config', {}).get('homeassistant', {})
    if ha_cfg.get('use_flag_sync') or ha_cfg.get('flag_entity'):
        print(f"   üö© Sincronizaci√≥n por bandera activa: {ha_cfg.get('flag_entity', 'N/A')} (esperar ON ‚Üí leer sensores ‚Üí modelo ‚Üí actuadores ‚Üí OFF)")
    
    # 2. Crear entorno
    env = create_production_env(config)
    
    # 3. Aplicar wrappers
    env = apply_wrappers(env, config)
    
    # 4. Cargar o crear modelo
    if model_path and os.path.exists(model_path):
        # Cargar modelo primero para obtener su action_space y usarlo en el entorno
        print(f"\nüîç Verificando action_space del modelo para ajustar entorno...")
        try:
            temp_model = SAC.load(model_path, device='cpu')
            model_action_space = temp_model.action_space
            print(f"   üìä Modelo tiene action_space: {model_action_space.shape[0]} dimensiones")
            print(f"      Low:  {model_action_space.low}")
            print(f"      High: {model_action_space.high}")
            
            # Si el entorno tiene un action_space diferente, actualizarlo al del modelo
            if env.action_space.shape[0] != model_action_space.shape[0]:
                print(f"\n   ‚ö†Ô∏è  El entorno tiene {env.action_space.shape[0]} dimensiones")
                print(f"   ‚ö†Ô∏è  pero el modelo necesita {model_action_space.shape[0]} dimensiones")
                print(f"   üîÑ Actualizando configuraci√≥n para usar action_space del modelo...")
                
                # Actualizar la configuraci√≥n para usar el action_space del modelo
                if 'env_config' not in config:
                    config['env_config'] = {}
                config['env_config']['action_space'] = {
                    'low': model_action_space.low.tolist(),
                    'high': model_action_space.high.tolist()
                }
                print(f"   ‚úÖ Configuraci√≥n actualizada")
                
                # Recrear el entorno con el action_space correcto
                env.close()
                env = create_production_env(config)
                env = apply_wrappers(env, config)
                print(f"   ‚úÖ Entorno recreado con action_space del modelo ({model_action_space.shape[0]} dimensiones)")
        except Exception as e:
            print(f"   ‚ö†Ô∏è  No se pudo verificar action_space del modelo: {e}")
            import traceback
            traceback.print_exc()
        
        # Ahora cargar el modelo normalmente
        model, env = load_production_model(model_path, config, env)
    else:
        print("\n‚ö†Ô∏è  No se proporcion√≥ modelo, creando uno nuevo...")
        if HAS_SB3:
            # Crear nuevo modelo
            algo_name = config.get('algorithm', {}).get('name', 'SAC')
            
            if algo_name == 'SAC':
                model = SAC('MlpPolicy', env, verbose=1, 
                          **config.get('algorithm_params', {}))
            elif algo_name == 'PPO':
                model = PPO('MlpPolicy', env, verbose=1,
                          **config.get('algorithm_params', {}))
            elif algo_name == 'TD3':
                model = TD3('MlpPolicy', env, verbose=1,
                          **config.get('algorithm_params', {}))
            elif algo_name == 'DDPG':
                model = DDPG('MlpPolicy', env, verbose=1,
                           **config.get('algorithm_params', {}))
            else:
                model = SAC('MlpPolicy', env, verbose=1)
            
            print(f"‚úÖ Nuevo modelo creado: {algo_name}")
        else:
            print("‚ùå No se puede crear modelo sin Stable Baselines3")
            return
    
    # 5. Ejecutar prueba inicial (opcional)
    if config.get('run_initial_test', True):
        run_production_test(model, env, config)
    
    # 6. Entrenamiento online
    if config.get('enable_online_training', True):
        model = train_online(model, env, config)
    
    # 7. Ejecutar prueba final
    if config.get('run_final_test', True):
        run_production_test(model, env, config)
    
    # 8. Guardar modelo final
    final_save_path = f"./models/production_model_{datetime.now().strftime('%Y%m%d_%H%M%S')}.zip"
    os.makedirs(os.path.dirname(final_save_path), exist_ok=True)
    model.save(final_save_path)
    print(f"\nüíæ Modelo final guardado en: {final_save_path}")
    
    # 9. Cerrar entorno
    env.close()
    print("\n‚úÖ Sistema de aprendizaje online finalizado")
    print(f"   Hora fin: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")

# ============ EJECUCI√ìN DESDE L√çNEA DE COMANDOS ============
if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description='Sistema de aprendizaje online en producci√≥n')
    parser.add_argument('--config', type=str, required=True, help='Ruta al archivo de configuraci√≥n YAML')
    parser.add_argument('--model', type=str, help='Ruta al modelo preentrenado (.zip)')
    parser.add_argument('--test-only', action='store_true', help='Solo ejecutar prueba sin entrenar')
    parser.add_argument('--no-train', action='store_true', help='No realizar entrenamiento online')
    
    args = parser.parse_args()
    
    # Validar archivo de configuraci√≥n
    if not os.path.exists(args.config):
        print(f"‚ùå Archivo de configuraci√≥n no encontrado: {args.config}")
        sys.exit(1)
    
    # Validar modelo si se proporciona
    if args.model and not os.path.exists(args.model):
        print(f"‚ö†Ô∏è  Modelo no encontrado: {args.model}")
        args.model = None
    
    # Modificar configuraci√≥n seg√∫n argumentos
    config = load_config(args.config)
    
    if args.test_only:
        config['enable_online_training'] = False
        config['run_initial_test'] = True
        config['run_final_test'] = True
    elif args.no_train:
        config['enable_online_training'] = False
    
    # Ejecutar
    try:
        run_online_learning(args.config, args.model)
    except KeyboardInterrupt:
        print("\n\n‚ö†Ô∏è  Programa interrumpido por el usuario")
    except Exception as e:
        print(f"\n‚ùå Error durante ejecuci√≥n: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)